{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cdafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "def prediction(model, image, patch_size = (2200,1550)):\n",
    "    segm_img = torch.zeros(image.shape)  ##this is 2d, only one channel.  do i want (x,y,1)?\n",
    "    patch_w, patch_h = patch_size\n",
    "    #print(segm_img.shape)\n",
    "    patch_num=1\n",
    "    img_w_shape = image.shape[2]\n",
    "    img_h_shape = image.shape[3]\n",
    "    for i in range(0, img_w_shape, patch_w):   #step by the width of the patch\n",
    "        for j in range(0, img_h_shape, patch_h):  #step by height of patch\n",
    "            w_start = i\n",
    "            h_start = j\n",
    "            w_end = i+patch_w\n",
    "            h_end = j+patch_h\n",
    "            #print(img_w_shape,w_end,img_h_shape,h_end )\n",
    "            if w_end > img_w_shape:\n",
    "              w_end = img_w_shape\n",
    "              w_start = w_end - patch_w\n",
    "            if h_end > img_h_shape:\n",
    "              h_end = img_h_shape\n",
    "              h_start = h_end - patch_h\n",
    "\n",
    "\n",
    "            single_patch = image[:,:,w_start:w_end, h_start:h_end]\n",
    "            single_patch_shape = single_patch.shape\n",
    "            \n",
    "            \n",
    "            single_patch_prediction = single_patch ##single_patch is input to model\n",
    "            \n",
    "            segm_img[:,:,w_start:w_start+patch_w, h_start:h_start+patch_h] += single_patch_prediction\n",
    "          \n",
    "            #print(\"Finished processing patch number \", patch_num, \" at position \", i,j)\n",
    "            patch_num+=1\n",
    "    return segm_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2191a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_depth_image(depth_img, scale = 1, show_image = True, target_mask = False):\n",
    "  \"\"\"\n",
    "  makes the depth predictions into an image.\n",
    "  blue is farthest away, red is closest.  \n",
    "  \n",
    "  depth_img is either a target or a model output.  should be 1 channel\n",
    "  scale is a constant and is multiplied by the inputs.  just used to map to rgb\n",
    "  show_image will print the image for convenience.  \n",
    "  target_mask if it is set to true, it makes all the pixels that are exaclty zero white:\n",
    "  per the competition those pixels are errors, not actual depth readings.  \n",
    "  \"\"\"\n",
    "  b = depth_img\n",
    "  rel_max = torch.max(b)\n",
    "  g = torch.zeros_like(depth_img)\n",
    "  r = rel_max - b\n",
    "  pic = torch.cat((r,g,b),axis = 1)\n",
    "\n",
    "  if target_mask:\n",
    "    #for targets all 0s are invalid, make em white\n",
    "    target_mask = depth_img == 0\n",
    "    target_mask = target_mask.expand(-1,3,-1,-1)\n",
    "    pic[target_mask] = 255\n",
    "\n",
    "  pic = pic.squeeze().permute(1,2,0) * scale\n",
    "  \n",
    "  if show_image:\n",
    "    plt.imshow(pic)\n",
    "  return pic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
