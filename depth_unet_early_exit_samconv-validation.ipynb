{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f585dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/cs585bp/nkono/IVC_MDE\n"
     ]
    }
   ],
   "source": [
    "cd ../../nkono/IVC_MDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b88fa9",
   "metadata": {
    "id": "38b88fa9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "from torchvision import models\n",
    "import tqdm\n",
    "\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize, Compose, ToPILImage, ToTensor\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "from torchvision.transforms import RandomCrop\n",
    "#os.chdir('/projectnb/cs585bp/jkoh/')\n",
    "#from SILog import SILogLoss\n",
    "#os.chdir('/projectnb/cs585bp/krishna/project/')\n",
    "from SILog import SILogLoss\n",
    "OUT_SIZE = (2200, 1550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff9dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0389eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_match(sam):\n",
    "  \"\"\"\n",
    "  roll axes 8 times, and stack\n",
    "  then truth test with original\n",
    "  \"\"\" \n",
    "  \n",
    "    #pad out the image so the indexing works\n",
    "  padding = (1, 1, 1, 1, 0, 0)  # (padding_left, padding_right, padding_top, padding_bottom, padding_front, padding_back)\n",
    "  #print(sam.shape)\n",
    "  sam = F.pad(sam, padding, mode='constant', value=-1) #negative 1 so it never matches. \n",
    "  #print(sam.shape)\n",
    "    \n",
    "  surrounding_pixels = torch.cat([\n",
    "    torch.roll(sam, shifts=(-1, -1), dims=(2, 3))  \n",
    "    ,torch.roll(sam, shifts=(-1, 0), dims=(2, 3)) \n",
    "    ,torch.roll(sam, shifts=(-1, 1), dims=(2, 3))  \n",
    "\n",
    "    ,torch.roll(sam, shifts=(0, -1), dims=(2, 3))  \n",
    "    ,torch.roll(sam, shifts=(0, 1), dims=(2, 3))  \n",
    "\n",
    "    ,torch.roll(sam, shifts=(1, -1), dims=(2, 3))  \n",
    "    ,torch.roll(sam, shifts=(1, 0), dims=(2, 3))  \n",
    "    ,torch.roll(sam, shifts=(1, 1), dims=(2, 3))   \n",
    "  ], axis = 1)\n",
    "  \n",
    " \n",
    "\n",
    "  return (sam == surrounding_pixels)[:,:,1:-1,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d07b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_difs(output, target):\n",
    "    #roll\n",
    "    out_roll_2 = torch.roll(output,1,2)\n",
    "    out_roll_3 = torch.roll(output,1,3)\n",
    "    tar_roll_2 = torch.roll(target,1,2)\n",
    "    tar_roll_3 = torch.roll(target,1,3)\n",
    "    \n",
    "    #get gradients\n",
    "    out_grad_2 = output - out_roll_2\n",
    "    out_grad_3 = output - out_roll_3\n",
    "    tar_grad_2 = target - tar_roll_2\n",
    "    tar_grad_3 = target - tar_roll_3\n",
    "    \n",
    "    #gradient diffs\n",
    "    grad_2_dif = (out_grad_2 - tar_grad_2)**2\n",
    "    grad_3_dif = (out_grad_3 - tar_grad_3)**2\n",
    "    \n",
    "    grad_dif = torch.mean(grad_2_dif + grad_3_dif)\n",
    "    \n",
    "    return grad_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pni49kG_K-sG",
   "metadata": {
    "id": "pni49kG_K-sG"
   },
   "outputs": [],
   "source": [
    "## a pretty function, but it doesn't work :(\n",
    "    ##our model doesn't do exactly half upscaling because the dimensions aren't powers of 2. using resize instead\n",
    "\n",
    "def reduce_tensor(input_tensor, scale_down = 2, mode = \"mean\"):\n",
    "    \"\"\"\n",
    "    mode can be mean, max, or min\n",
    "    \"\"\"\n",
    "    batch, _, input_w, input_h = input_tensor.shape\n",
    "    output_w = int(input_w/scale_down)\n",
    "    output_h = int(input_h/scale_down)\n",
    "\n",
    "    output_tensor = input_tensor.view(batch,scale_down**2,output_w,output_h)\n",
    "    \n",
    "    if mode == \"mean\":\n",
    "      output_tensor = output_tensor.mean(dim=1)\n",
    "\n",
    "    #if mode == \"min\":\n",
    "     # output_tensor = torch.min(output_tensor,1)\n",
    "\n",
    "    #if mode == \"max\":\n",
    "     # output_tensor = output_tensor.max(dim=1)\n",
    "    \n",
    "    output_tensor = output_tensor.unsqueeze(1)\n",
    "    \n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d30c88ba",
   "metadata": {
    "id": "d30c88ba"
   },
   "outputs": [],
   "source": [
    "class MonocularDepthDataset(Dataset):\n",
    "    def __init__(self, df, in_transform=None,out_transform = None ):\n",
    "        self.df = df\n",
    "        self.in_transform = in_transform\n",
    "        self.out_transform = out_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.df.iloc[idx]['image']\n",
    "        depth_path = self.df.iloc[idx]['depth']\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        depth = Image.open(depth_path)\n",
    "        \n",
    "        sam_path = self.df.iloc[idx]['image']\n",
    "        sam_path = sam_path.split('/')\n",
    "        sam_path[0] = 'sam_outputs/mask/'\n",
    "        sam_path = ''.join(sam_path)\n",
    "        sam = Image.open(sam_path)\n",
    "        \n",
    "\n",
    "        #torch.Size([1, 1, 2200, 1550])\n",
    "        #torch.Size([1, 1, 1098, 773]) #d_0\n",
    "        #torch.Size([1, 1, 550, 388]) #d_1\n",
    "        #torch.Size([1, 1, 275, 194]) #d_2\n",
    "        #torch.Size([1, 1, 138, 97]) #d_3\n",
    "        #torch.Size([1, 1, 69, 49]) #d_4\n",
    "        d_0 = depth.resize((773, 1098))#((1098, 773))#(depth)\n",
    "        d_1 = depth.resize((388, 550))#((550, 388))#(depth)\n",
    "        d_2 = depth.resize((194, 275))#((275, 194))#(depth)\n",
    "        d_3 = depth.resize((97, 138))#((138, 97))#(depth)\n",
    "        d_4 = depth.resize((49,69))#((69, 49))#(depth)\n",
    "\n",
    "\n",
    "\n",
    "        if self.in_transform:\n",
    "            image = self.in_transform(image)\n",
    "            sam = self.in_transform(sam)\n",
    "        if self.out_transform:\n",
    "            depth = self.out_transform(depth)\n",
    "            to_tensor = transforms.ToTensor()\n",
    "            d_0 = to_tensor(d_0)\n",
    "            d_1 = to_tensor(d_1)\n",
    "            d_2 = to_tensor(d_2)\n",
    "            d_3 = to_tensor(d_3)\n",
    "            d_4 = to_tensor(d_4)\n",
    "        #print(depth_path,depth.shape)\n",
    "        #print('funct shape: ', sam.shape)\n",
    "        #sam = neighbor_match(sam)\n",
    "            \n",
    "        return image, depth, d_0, d_1, d_2, d_3, d_4, sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5adc8983",
   "metadata": {
    "id": "5adc8983"
   },
   "outputs": [],
   "source": [
    "def conv_relu_block(in_channel,out_channel,kernel,padding):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_channel,out_channel, kernel_size = kernel, padding=padding),\n",
    "            nn.ReLU()) #nn.ReLU(inplace=True) #nn.Ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a171402",
   "metadata": {
    "id": "9a171402"
   },
   "outputs": [],
   "source": [
    "class vanilla_unet_early_exit(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.input_1 = conv_relu_block(3,3,3,1) ##grayscale inputs\n",
    "        #self.input_2 = conv_relu_block(64, 64, 3, 1) #no extra channels\n",
    "        \n",
    "        self.samconv = conv_relu_block(8,2,1,0) #this has to match samid below, unless it is zero index? not sure\n",
    "        \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.l0 = nn.Sequential(*self.base_layers[:3])\n",
    "        self.U0_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up0 = conv_relu_block(64 + 256, 128, 3, 0)\n",
    "\n",
    "        self.l1 = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.U1_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up1 = conv_relu_block(64 + 256, 256, 3, 1)\n",
    "\n",
    "        self.l2 = self.base_layers[5]\n",
    "        self.U2_conv = conv_relu_block(128, 128, 1, 0)\n",
    "        self.conv_up2 = conv_relu_block(128 + 512, 256, 3, 1)\n",
    "\n",
    "        self.l3 = self.base_layers[6]\n",
    "        self.U3_conv = conv_relu_block(256, 256, 1, 0)\n",
    "        self.conv_up3 = conv_relu_block(256 + 512, 512, 3, 1)\n",
    "\n",
    "        self.l4 = self.base_layers[7]\n",
    "        self.U4_conv = conv_relu_block(512, 512, 1, 0)\n",
    "\n",
    "        self.conv_up4 = conv_relu_block(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.out4 = nn.Conv2d(128, n_class, 1)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x, sam):\n",
    "        #max_sam_id = 100\n",
    "        #print('starting shape: ', sam.shape)\n",
    "        #sam = torch.nn.functional.one_hot(sam.to(torch.int64), num_classes= max_sam_id)\n",
    "        #sam = torch.moveaxis(sam,4,2)\n",
    "        #sam = torch.squeeze(sam,1) #now it is b x max_sam_id x h x w\n",
    "\n",
    "        sam = self.samconv(sam.float())\n",
    "        \n",
    "        x = torch.cat([x,sam], axis = 1)\n",
    "        x = self.input_1(x)\n",
    "        \n",
    "        #print(x.shape,'x')\n",
    "         #concat on channel\n",
    "        #x_one = self.input_2(x_one)\n",
    "        block0 = self.l0(x)\n",
    "        block1 = self.l1(block0)\n",
    "        block2 = self.l2(block1)\n",
    "        block3 = self.l3(block2)\n",
    "        block4 = self.l4(block3)\n",
    "\n",
    "        block4 = self.U4_conv(block4)\n",
    "        b4_out = block4[:,-1,:,:].unsqueeze(1)\n",
    "        #print(b4_out.shape, \"block_4\")\n",
    "        x = nn.Upsample(size = (138,97), mode='bilinear', align_corners=True)(block4)\n",
    "        block3 = self.U3_conv(block3)\n",
    "        \n",
    "        x = torch.cat([x, block3], axis=1)\n",
    "        \n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        b3_out = x[:,-1,:,:].unsqueeze(1)\n",
    "        #print(b3_out.shape, \"block3\")\n",
    "        x = nn.Upsample(size = (275,194), mode='bilinear', align_corners=True)(x)\n",
    "        \n",
    "        block2 = self.U2_conv(block2)\n",
    "        \n",
    "        #print(x.shape, block2.shape)\n",
    "        x = torch.cat([x, block2], axis=1)\n",
    "        \n",
    "        x = self.conv_up2(x)\n",
    "        b2_out = x[:,-1,:,:].unsqueeze(1)\n",
    "        #print(b2_out.shape, \"block2\")\n",
    "        x = nn.Upsample(size = (550,388), mode='bilinear', align_corners=True)(x)\n",
    "        block1 = self.U1_conv(block1)\n",
    "        #print(x.shape, block1.shape)\n",
    "        \n",
    "        x = torch.cat([x, block1], axis=1)\n",
    "        \n",
    "        x = self.conv_up1(x)\n",
    "        b1_out = x[:,-1,:,:].unsqueeze(1)\n",
    "        #print(b1_out.shape, \"block1\")\n",
    "        x = nn.Upsample(size = (1100, 775), mode='bilinear', align_corners=True)(x) \n",
    "        block0 = self.U0_conv(block0)\n",
    "\n",
    "        #print(x.shape, block0.shape)\n",
    "        x = torch.cat([x, block0], axis=1)\n",
    "\n",
    "        \n",
    "        x = self.conv_up0(x)\n",
    "        b0_out = x[:,-1,:,:].unsqueeze(1)\n",
    "        #print(b0_out.shape, \"block0\")\n",
    "        out4 = self.out4(x)\n",
    "\n",
    "        out4_upsampled = F.interpolate(out4, size=OUT_SIZE, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        out = out4_upsampled\n",
    "        \n",
    "        \n",
    "        return out, b0_out, b1_out, b2_out, b3_out, b4_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d68a8a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675b5d0e1b2548cdaf8254e5d1c47d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  tensor(69.5564, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_path = \"../../jkoh/inputs/\"\n",
    "y_path = '../../jkoh/depth_annotations/'\n",
    "val_path = '../../krishna/project/val.csv'\n",
    "\n",
    "class MonocularDepthDataset(Dataset):\n",
    "    def __init__(self, df, in_transform=None,out_transform = None ):\n",
    "        self.df = df\n",
    "        self.in_transform = in_transform\n",
    "        self.out_transform = out_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.df.iloc[idx]['image']\n",
    "        depth_path = self.df.iloc[idx]['depth']\n",
    "\n",
    "        image = Image.open(in_path + image_path)\n",
    "        depth = Image.open(y_path + depth_path)\n",
    "        \n",
    "        sam_path = self.df.iloc[idx]['image']\n",
    "        sam_path = '../../krishna/project/sam_outputs/val_mask/'+ sam_path\n",
    "        sam = Image.open(sam_path)\n",
    "        \n",
    "\n",
    "        #torch.Size([1, 1, 2200, 1550])\n",
    "        #torch.Size([1, 1, 1098, 773]) #d_0\n",
    "        #torch.Size([1, 1, 550, 388]) #d_1\n",
    "        #torch.Size([1, 1, 275, 194]) #d_2\n",
    "        #torch.Size([1, 1, 138, 97]) #d_3\n",
    "        #torch.Size([1, 1, 69, 49]) #d_4\n",
    "        d_0 = depth.resize((773, 1098))#((1098, 773))#(depth)\n",
    "        d_1 = depth.resize((388, 550))#((550, 388))#(depth)\n",
    "        d_2 = depth.resize((194, 275))#((275, 194))#(depth)\n",
    "        d_3 = depth.resize((97, 138))#((138, 97))#(depth)\n",
    "        d_4 = depth.resize((49,69))#((69, 49))#(depth)\n",
    "\n",
    "\n",
    "\n",
    "        if self.in_transform:\n",
    "            image = self.in_transform(image)\n",
    "            sam = self.in_transform(sam)\n",
    "        if self.out_transform:\n",
    "            depth = self.out_transform(depth)\n",
    "            to_tensor = transforms.ToTensor()\n",
    "            d_0 = to_tensor(d_0)\n",
    "            d_1 = to_tensor(d_1)\n",
    "            d_2 = to_tensor(d_2)\n",
    "            d_3 = to_tensor(d_3)\n",
    "            d_4 = to_tensor(d_4)\n",
    "        #print(depth_path,depth.shape)\n",
    "        #print('funct shape: ', sam.shape)\n",
    "        #sam = neighbor_match(sam)\n",
    "            \n",
    "        return image, depth, d_0, d_1, d_2, d_3, d_4, sam\n",
    "    \n",
    "    \n",
    "device = 'cuda' \n",
    "\n",
    "model = vanilla_unet_early_exit(n_class=1)\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('early_exit_final_final_40.pt'))\n",
    "model = model.to(device)\n",
    "\n",
    "#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "in_transform = transforms.Compose([\n",
    "    transforms.Resize(OUT_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "out_transform = transforms.Compose([\n",
    "    transforms.Resize(OUT_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "df = pd.read_csv(val_path)\n",
    "test_dataset = MonocularDepthDataset(df, in_transform = in_transform, out_transform = out_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "criterion = SILogLoss()\n",
    "\n",
    "\n",
    "test_loss = 0.0\n",
    "metric = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, depths, d_0, d_1, d_2, d_3, d_4, sam in tqdm.notebook.tqdm(test_dataloader):\n",
    "        images = images.to(device)\n",
    "        depths = depths.to(device)\n",
    "        sam = neighbor_match(sam)\n",
    "        sam = sam.to(device)\n",
    "        d_0 = d_0.to(device)\n",
    "        d_1 = d_1.to(device)\n",
    "        d_2 = d_2.to(device)\n",
    "        d_3 = d_3.to(device)\n",
    "        d_4 = d_4.to(device)\n",
    "\n",
    "        out, b0_out, b1_out, b2_out, b3_out, b4_out = model(images, sam)\n",
    "        loss_pred = criterion(out, depths) ##output,targets\n",
    "        loss_grad = gradient_difs(out, depths)\n",
    "        loss_0 = criterion(b0_out, d_0)\n",
    "        loss_1 = criterion(b1_out, d_1)\n",
    "        loss_2 = criterion(b2_out, d_2)\n",
    "        loss_3 = criterion(b3_out, d_3)\n",
    "        loss_4 = criterion(b4_out, d_4)\n",
    "        loss = loss_pred + loss_0 + loss_1 + loss_2 + loss_3 + loss_4 + loss_grad**0.5\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        metric+=criterion(out,depths)\n",
    "        #print(metric,loss_0)\n",
    "        \n",
    "\n",
    "metric /= len(test_dataloader)\n",
    "print('Test Loss: ', metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d756ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_unet_early_exit(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.input_1 = conv_relu_block(3,3,3,1) ##grayscale inputs\n",
    "        #self.input_2 = conv_relu_block(64, 64, 3, 1) #no extra channels\n",
    "        \n",
    "        self.samconv = conv_relu_block(50,2,1,0) #this has to match samid below, unless it is zero index? not sure\n",
    "        \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.l0 = nn.Sequential(*self.base_layers[:3])\n",
    "        self.U0_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up0 = conv_relu_block(64 + 256, 128, 3, 0)\n",
    "\n",
    "        self.l1 = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.U1_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up1 = conv_relu_block(64 + 256, 256, 3, 1)\n",
    "\n",
    "        self.l2 = self.base_layers[5]\n",
    "        self.U2_conv = conv_relu_block(128, 128, 1, 0)\n",
    "        self.conv_up2 = conv_relu_block(128 + 512, 256, 3, 1)\n",
    "\n",
    "        self.l3 = self.base_layers[6]\n",
    "        self.U3_conv = conv_relu_block(256, 256, 1, 0)\n",
    "        self.conv_up3 = conv_relu_block(256 + 512, 512, 3, 1)\n",
    "\n",
    "        self.l4 = self.base_layers[7]\n",
    "        self.U4_conv = conv_relu_block(512, 512, 1, 0)\n",
    "\n",
    "        self.conv_up4 = conv_relu_block(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.out4 = nn.Conv2d(128, n_class, 1)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x, sam):\n",
    "        max_sam_id = 50\n",
    "        #print('starting shape: ', sam.shape)\n",
    "        sam = torch.nn.functional.one_hot(sam.to(torch.int64), num_classes= max_sam_id)\n",
    "        sam = torch.moveaxis(sam,4,2)\n",
    "        sam = torch.squeeze(sam,1) #now it is b x max_sam_id x h x w\n",
    "\n",
    "        sam = self.samconv(sam.float())\n",
    "        \n",
    "        x = torch.cat([x,sam], axis = 1)\n",
    "        x = self.input_1(x)\n",
    "        \n",
    "        #print(x.shape,'x')\n",
    "         #concat on channel\n",
    "        #x_one = self.input_2(x_one)\n",
    "        block0 = self.l0(x)\n",
    "        block1 = self.l1(block0)\n",
    "        block2 = self.l2(block1)\n",
    "        block3 = self.l3(block2)\n",
    "        block4 = self.l4(block3)\n",
    "\n",
    "        block4 = self.U4_conv(block4)\n",
    "        b4_out = block4[:,-1,:,:].unsqueeze(1)\n",
    "        #print(b4_out.shape, \"block_4\")\n",
    "        x = nn.Upsample(size = (138,97), mode='bilinear', align_corners=True)(block4)\n",
    "        block3 = self.U3_conv(block3)\n",
    "        \n",
    "        x = torch.cat([x, block3], axis=1)\n",
    "        \n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        b3_out = x[:,-1,:,:].unsqueeze(1)\n",
    "        #print(b3_out.shape, \"block3\")\n",
    "        x = nn.Upsample(size = (275,194), mode='bilinear', align_corners=True)(x)\n",
    "        \n",
    "        block2 = self.U2_conv(block2)\n",
    "        \n",
    "        #print(x.shape, block2.shape)\n",
    "        x = torch.cat([x, block2], axis=1)\n",
    "        \n",
    "        x = self.conv_up2(x)\n",
    "        b2_out = x[:,-1,:,:].unsqueeze(1)\n",
    "        #print(b2_out.shape, \"block2\")\n",
    "        x = nn.Upsample(size = (550,388), mode='bilinear', align_corners=True)(x)\n",
    "        block1 = self.U1_conv(block1)\n",
    "        #print(x.shape, block1.shape)\n",
    "        \n",
    "        x = torch.cat([x, block1], axis=1)\n",
    "        \n",
    "        x = self.conv_up1(x)\n",
    "        b1_out = x[:,-1,:,:].unsqueeze(1)\n",
    "        #print(b1_out.shape, \"block1\")\n",
    "        x = nn.Upsample(size = (1100, 775), mode='bilinear', align_corners=True)(x) \n",
    "        block0 = self.U0_conv(block0)\n",
    "\n",
    "        #print(x.shape, block0.shape)\n",
    "        x = torch.cat([x, block0], axis=1)\n",
    "\n",
    "        \n",
    "        x = self.conv_up0(x)\n",
    "        b0_out = x[:,-1,:,:].unsqueeze(1)\n",
    "        #print(b0_out.shape, \"block0\")\n",
    "        out4 = self.out4(x)\n",
    "\n",
    "        out4_upsampled = F.interpolate(out4, size=OUT_SIZE, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        out = out4_upsampled\n",
    "        \n",
    "        \n",
    "        return out, b0_out, b1_out, b2_out, b3_out, b4_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a856d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2f1f6169994a24b1b24a15e50a5f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  tensor(69.5564, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_path = \"../../jkoh/inputs/\"\n",
    "y_path = '../../jkoh/depth_annotations/'\n",
    "val_path = '../../krishna/project/val.csv'\n",
    "\n",
    "class MonocularDepthDataset(Dataset):\n",
    "    def __init__(self, df, in_transform=None,out_transform = None ):\n",
    "        self.df = df\n",
    "        self.in_transform = in_transform\n",
    "        self.out_transform = out_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.df.iloc[idx]['image']\n",
    "        depth_path = self.df.iloc[idx]['depth']\n",
    "\n",
    "        image = Image.open(in_path + image_path)\n",
    "        depth = Image.open(y_path + depth_path)\n",
    "        \n",
    "        sam_path = self.df.iloc[idx]['image']\n",
    "        sam_path = '../../krishna/project/sam_outputs/val_mask/'+ sam_path\n",
    "        sam = Image.open(sam_path)\n",
    "        \n",
    "\n",
    "        #torch.Size([1, 1, 2200, 1550])\n",
    "        #torch.Size([1, 1, 1098, 773]) #d_0\n",
    "        #torch.Size([1, 1, 550, 388]) #d_1\n",
    "        #torch.Size([1, 1, 275, 194]) #d_2\n",
    "        #torch.Size([1, 1, 138, 97]) #d_3\n",
    "        #torch.Size([1, 1, 69, 49]) #d_4\n",
    "        d_0 = depth.resize((773, 1098))#((1098, 773))#(depth)\n",
    "        d_1 = depth.resize((388, 550))#((550, 388))#(depth)\n",
    "        d_2 = depth.resize((194, 275))#((275, 194))#(depth)\n",
    "        d_3 = depth.resize((97, 138))#((138, 97))#(depth)\n",
    "        d_4 = depth.resize((49,69))#((69, 49))#(depth)\n",
    "\n",
    "\n",
    "\n",
    "        if self.in_transform:\n",
    "            image = self.in_transform(image)\n",
    "            sam = self.in_transform(sam)\n",
    "        if self.out_transform:\n",
    "            depth = self.out_transform(depth)\n",
    "            to_tensor = transforms.ToTensor()\n",
    "            d_0 = to_tensor(d_0)\n",
    "            d_1 = to_tensor(d_1)\n",
    "            d_2 = to_tensor(d_2)\n",
    "            d_3 = to_tensor(d_3)\n",
    "            d_4 = to_tensor(d_4)\n",
    "        #print(depth_path,depth.shape)\n",
    "        #print('funct shape: ', sam.shape)\n",
    "        #sam = neighbor_match(sam)\n",
    "            \n",
    "        return image, depth, d_0, d_1, d_2, d_3, d_4, sam\n",
    "    \n",
    "    \n",
    "device = 'cuda' \n",
    "\n",
    "model = vanilla_unet_early_exit(n_class=1)\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('early_exit_unet_sam_grad_diffs.pt'))\n",
    "#model.load_state_dict(torch.load('early_exit_final_final_40.pt'))\n",
    "model = model.to(device)\n",
    "\n",
    "#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "in_transform = transforms.Compose([\n",
    "    transforms.Resize(OUT_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "out_transform = transforms.Compose([\n",
    "    transforms.Resize(OUT_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "df = pd.read_csv(val_path)\n",
    "test_dataset = MonocularDepthDataset(df, in_transform = in_transform, out_transform = out_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "criterion = SILogLoss()\n",
    "\n",
    "\n",
    "test_loss = 0.0\n",
    "metric = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, depths, d_0, d_1, d_2, d_3, d_4, sam in tqdm.notebook.tqdm(test_dataloader):\n",
    "        images = images.to(device)\n",
    "        depths = depths.to(device)\n",
    "        #sam = neighbor_match(sam)\n",
    "        sam = sam.to(device)\n",
    "        d_0 = d_0.to(device)\n",
    "        d_1 = d_1.to(device)\n",
    "        d_2 = d_2.to(device)\n",
    "        d_3 = d_3.to(device)\n",
    "        d_4 = d_4.to(device)\n",
    "\n",
    "        out, b0_out, b1_out, b2_out, b3_out, b4_out = model(images, sam)\n",
    "        loss_pred = criterion(out, depths) ##output,targets\n",
    "        loss_grad = gradient_difs(out, depths)\n",
    "        loss_0 = criterion(b0_out, d_0)\n",
    "        loss_1 = criterion(b1_out, d_1)\n",
    "        loss_2 = criterion(b2_out, d_2)\n",
    "        loss_3 = criterion(b3_out, d_3)\n",
    "        loss_4 = criterion(b4_out, d_4)\n",
    "        loss = loss_pred + loss_0 + loss_1 + loss_2 + loss_3 + loss_4 + loss_grad**0.5\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        metric+=criterion(out,depths)\n",
    "        #print(metric,loss_0)\n",
    "        \n",
    "\n",
    "metric /= len(test_dataloader)\n",
    "print('Test Loss: ', metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55498938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
