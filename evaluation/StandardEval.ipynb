{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19442c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from SILog import SILogLoss\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import inference as I\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "OUT_SIZE = (2200, 1550)\n",
    "in_transform = transforms.Compose([\n",
    "    transforms.Resize(OUT_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "out_transform = transforms.Compose([\n",
    "    transforms.Resize(OUT_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "class MonocularDepthDataset(Dataset):\n",
    "    def __init__(self, df, in_transform=None,out_transform = None ):\n",
    "        self.df = df\n",
    "        self.in_transform = in_transform\n",
    "        self.out_transform = out_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.df[idx][0]\n",
    "        depth_path = self.df[idx][1]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        depth = Image.open(depth_path)\n",
    "\n",
    "        if self.in_transform:\n",
    "            image = self.in_transform(image)\n",
    "        if self.out_transform:\n",
    "            depth = self.out_transform(depth)\n",
    "        #print(depth_path,depth.shape)\n",
    "        return image, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eebefc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu_block(in_channel,out_channel,kernel,padding):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_channel,out_channel, kernel_size = kernel, padding=padding),\n",
    "            nn.ReLU()) #nn.ReLU(inplace=True) #nn.Ge\n",
    "class vanilla_unet_full_nearest(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.input_1 = conv_relu_block(3,3,3,1) ##grayscale inputs\n",
    "        #self.input_2 = conv_relu_block(64, 64, 3, 1) #no extra channels\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.l0 = nn.Sequential(*self.base_layers[:3])\n",
    "        self.U0_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up0 = conv_relu_block(64 + 256, 128, 3, 0)\n",
    "\n",
    "        self.l1 = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.U1_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up1 = conv_relu_block(64 + 256, 256, 3, 1)\n",
    "\n",
    "        self.l2 = self.base_layers[5]\n",
    "        self.U2_conv = conv_relu_block(128, 128, 1, 0)\n",
    "        self.conv_up2 = conv_relu_block(128 + 512, 256, 3, 1)\n",
    "\n",
    "        self.l3 = self.base_layers[6]\n",
    "        self.U3_conv = conv_relu_block(256, 256, 1, 0)\n",
    "        self.conv_up3 = conv_relu_block(256 + 512, 512, 3, 1)\n",
    "\n",
    "        self.l4 = self.base_layers[7]\n",
    "        self.U4_conv = conv_relu_block(512, 512, 1, 0)\n",
    "\n",
    "        self.conv_up4 = conv_relu_block(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.out4 = nn.Conv2d(128, n_class, 1)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([x,x,x], axis = 1)\n",
    "        x = self.input_1(x)\n",
    "        \n",
    "        #print(x.shape,'x')\n",
    "         #concat on channel\n",
    "        #x_one = self.input_2(x_one)\n",
    "        block0 = self.l0(x)\n",
    "        block1 = self.l1(block0)\n",
    "        block2 = self.l2(block1)\n",
    "        block3 = self.l3(block2)\n",
    "        block4 = self.l4(block3)\n",
    "\n",
    "        block4 = self.U4_conv(block4)\n",
    "        #print(block4.shape)\n",
    "        x = nn.Upsample(size = (138,97), mode='bilinear', align_corners=True)(block4)\n",
    "        block3 = self.U3_conv(block3)\n",
    "        \n",
    "        x = torch.cat([x, block3], axis=1)\n",
    "        x = self.conv_up3(x)\n",
    "        \n",
    "        x = nn.Upsample(size = (275,194), mode='bilinear', align_corners=True)(x)\n",
    "        \n",
    "        block2 = self.U2_conv(block2)\n",
    "        \n",
    "        #print(x.shape, block2.shape)\n",
    "        x = torch.cat([x, block2], axis=1)\n",
    "        \n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = nn.Upsample(size = (550,388), mode='bilinear', align_corners=True)(x)\n",
    "        block1 = self.U1_conv(block1)\n",
    "        #print(x.shape, block1.shape)\n",
    "        \n",
    "        x = torch.cat([x, block1], axis=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = nn.Upsample(size = (1100, 775), mode='bilinear', align_corners=True)(x) \n",
    "        block0 = self.U0_conv(block0)\n",
    "\n",
    "        #print(x.shape, block0.shape)\n",
    "        x = torch.cat([x, block0], axis=1)\n",
    "        x = self.conv_up0(x)\n",
    "        out4 = self.out4(x)\n",
    "\n",
    "        #out4_upsampled = F.interpolate(out4, size=OUT_SIZE, mode='nearest', align_corners=True)\n",
    "        out4_upsampled = F.interpolate(out4, size=OUT_SIZE, mode='nearest')\n",
    "        \n",
    "        out = F.relu(out4_upsampled)\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a9332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "<ipython-input-3-1514d3078de8>:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for images, depths in tqdm.tqdm_notebook(val_dataloader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ec8985de0e421caf6941d21b01a7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.990882124724212\n"
     ]
    }
   ],
   "source": [
    "PATH = 'unet_full_relu_nearest.pt'\n",
    "device = torch.device(\"cuda\")\n",
    "model = vanilla_unet_full_nearest(n_class=1)\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
    "model.to(device)\n",
    "\n",
    "in_path = \"inputs/\"\n",
    "y_path = 'depth_annotations/'\n",
    "dir_list = os.listdir(in_path)\n",
    "d_paths = [(in_path+v,y_path+v) for v in dir_list]\n",
    "val_dataset = MonocularDepthDataset(d_paths, in_transform = in_transform, out_transform = out_transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, num_workers=4)\n",
    "\n",
    "\n",
    "criterion = SILogLoss()\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    for images, depths in tqdm.tqdm_notebook(val_dataloader):\n",
    "        images = images.to(device)\n",
    "        depths = depths.to(device)\n",
    "        outputs = model(images)\n",
    "        #print('out shape',outputs.shape)\n",
    "        loss = criterion(outputs, depths)\n",
    "        running_loss += loss.item()\n",
    "print(running_loss/len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1aa43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
