{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b88fa9",
   "metadata": {
    "id": "38b88fa9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "from torchvision import models\n",
    "import tqdm\n",
    "\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize, Compose, ToPILImage, ToTensor\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "#from kornia.filters import SpatialGradient\n",
    "\n",
    "import random\n",
    "from torchvision.transforms import RandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "YAFmAijBUCp3",
   "metadata": {
    "id": "YAFmAijBUCp3"
   },
   "outputs": [],
   "source": [
    "patch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d30c88ba",
   "metadata": {
    "id": "d30c88ba"
   },
   "outputs": [],
   "source": [
    "class MonocularDepthDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, crop_size=patch_size):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.df.iloc[idx]['image']\n",
    "        depth_path = self.df.iloc[idx]['depth']\n",
    "\n",
    "        image = Image.open(image_path) ##no rgb, takes grayscale\n",
    "        depth = Image.open(depth_path)\n",
    "\n",
    "        # randomly crop image and depth\n",
    "        i, j, h, w = RandomCrop.get_params(image, output_size=(self.crop_size, self.crop_size))\n",
    "        image = F.crop(image, i, j, h, w)\n",
    "        depth = F.crop(depth, i, j, h, w)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            depth = self.transform(depth)\n",
    "\n",
    "        return image, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2d69bc",
   "metadata": {
    "id": "dd2d69bc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gradient_loss_fn(gen_frames, gt_frames, alpha=1):\n",
    "    def gradient(x):\n",
    "        # idea from tf.image.image_gradients(image)\n",
    "        # https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/image_ops_impl.py#L3441-L3512\n",
    "        # x: (b,c,h,w), float32 or float64\n",
    "        # dx, dy: (b,c,h,w)\n",
    "\n",
    "        h_x = x.size()[-2]\n",
    "        w_x = x.size()[-1]\n",
    "        # gradient step=1\n",
    "        left = x\n",
    "        right = F.pad(x, [0, 1, 0, 0])[:, :, :, 1:]\n",
    "        top = x\n",
    "        bottom = F.pad(x, [0, 0, 0, 1])[:, :, 1:, :]\n",
    "\n",
    "        # dx, dy = torch.abs(right - left), torch.abs(bottom - top)\n",
    "        dx, dy = right - left, bottom - top \n",
    "        # dx will always have zeros in the last column, right-left\n",
    "        # dy will always have zeros in the last row,    bottom-top\n",
    "        dx[:, :, :, -1] = 0\n",
    "        dy[:, :, -1, :] = 0\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "    # gradient\n",
    "    gen_dx, gen_dy = gradient(gen_frames)\n",
    "    gt_dx, gt_dy = gradient(gt_frames)\n",
    "    #\n",
    "    grad_diff_x = torch.abs(gt_dx - gen_dx)\n",
    "    grad_diff_y = torch.abs(gt_dy - gen_dy)\n",
    "\n",
    "    # condense into one tensor and avg\n",
    "    return torch.mean(grad_diff_x ** alpha + grad_diff_y ** alpha)\n",
    "\n",
    "class DepthEstimationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(DepthEstimationLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def forward(self, pred_depth, true_depth):\n",
    "        pred_depth = torch.clamp(pred_depth, min=1e-8)\n",
    "        true_depth = torch.clamp(true_depth, min=1e-8)\n",
    "\n",
    "        # Scale-invariant MSE loss\n",
    "        diff = torch.log(pred_depth) - torch.log(true_depth)\n",
    "        mse_loss = torch.mean(diff**2)\n",
    "        scale_invariant_mse_loss = mse_loss - (self.alpha * (torch.sum(diff)**2)) / (true_depth.numel()**2)\n",
    "\n",
    "    \n",
    "\n",
    "        gradient_loss = gradient_loss_fn(pred_depth,true_depth,alpha=self.alpha)\n",
    "\n",
    "        total_loss = (scale_invariant_mse_loss + gradient_loss)/2\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5adc8983",
   "metadata": {
    "id": "5adc8983"
   },
   "outputs": [],
   "source": [
    "def conv_relu_block(in_channel,out_channel,kernel,padding):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_channel,out_channel, kernel_size = kernel, padding=padding),\n",
    "            nn.ReLU()) #nn.ReLU(inplace=True) #nn.Ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a171402",
   "metadata": {
    "id": "9a171402"
   },
   "outputs": [],
   "source": [
    "class vanilla_unet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.input_1 = conv_relu_block(3,3,3,1) ##grayscale inputs\n",
    "        #self.input_2 = conv_relu_block(64, 64, 3, 1) #no extra channels\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.l0 = nn.Sequential(*self.base_layers[:3])\n",
    "        self.U0_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up0 = conv_relu_block(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.l1 = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.U1_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up1 = conv_relu_block(64 + 256, 256, 3, 1)\n",
    "\n",
    "        self.l2 = self.base_layers[5]\n",
    "        self.U2_conv = conv_relu_block(128, 128, 1, 0)\n",
    "        self.conv_up2 = conv_relu_block(128 + 512, 256, 3, 1)\n",
    "\n",
    "        self.l3 = self.base_layers[6]\n",
    "        self.U3_conv = conv_relu_block(256, 256, 1, 0)\n",
    "        self.conv_up3 = conv_relu_block(256 + 512, 512, 3, 1)\n",
    "\n",
    "        self.l4 = self.base_layers[7]\n",
    "        self.U4_conv = conv_relu_block(512, 512, 1, 0)\n",
    "\n",
    "        self.conv_up4 = conv_relu_block(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.out4 = nn.Conv2d(128, n_class, 1)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([x,x,x], axis = 1)\n",
    "        x = self.input_1(x)\n",
    "        \n",
    "        #print(x.shape,'x')\n",
    "         #concat on channel\n",
    "        #x_one = self.input_2(x_one)\n",
    "        block0 = self.l0(x)\n",
    "        block1 = self.l1(block0)\n",
    "        block2 = self.l2(block1)\n",
    "        block3 = self.l3(block2)\n",
    "        block4 = self.l4(block3)\n",
    "\n",
    "        block4 = self.U4_conv(block4)\n",
    "        x = self.upsample(block4)\n",
    "\n",
    "        #print(block0.shape, block1.shape, block2.shape,block3.shape,block4.shape)\n",
    "        block3 = self.U3_conv(block3)\n",
    "        #print(x.shape, block3.shape)\n",
    "        x = torch.cat([x, block3], axis=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        block2 = self.U2_conv(block2)\n",
    "        x = torch.cat([x, block2], axis=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        block1 = self.U1_conv(block1)\n",
    "        x = torch.cat([x, block1], axis=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        block0 = self.U0_conv(block0)\n",
    "        x = torch.cat([x, block0], axis=1)\n",
    "        x = self.conv_up0(x)\n",
    "        out4 = self.out4(x)\n",
    "\n",
    "        out4_upsampled = F.interpolate(out4, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        relu = nn.ReLU()\n",
    "        out = relu(out4_upsampled)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b2a61e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79b2a61e",
    "outputId": "7e094bd1-ea9c-4437-d7f4-410df78aea12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "v = vanilla_unet(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cAWkYz_BCsCv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAWkYz_BCsCv",
    "outputId": "51998883-11b6-41bf-d1ed-489196ab132f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0757, 0.0544, 0.0331,  ..., 0.0000, 0.0217, 0.0548],\n",
       "          [0.0869, 0.0730, 0.0590,  ..., 0.0000, 0.0265, 0.0581],\n",
       "          [0.0982, 0.0916, 0.0849,  ..., 0.0013, 0.0313, 0.0613],\n",
       "          ...,\n",
       "          [0.0581, 0.0790, 0.0999,  ..., 0.0000, 0.0202, 0.0585],\n",
       "          [0.0515, 0.0640, 0.0765,  ..., 0.0000, 0.0000, 0.0243],\n",
       "          [0.0449, 0.0490, 0.0532,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0300, 0.0226, 0.0152],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0288, 0.0335, 0.0382],\n",
       "          [0.0429, 0.0254, 0.0079,  ..., 0.0277, 0.0444, 0.0612],\n",
       "          ...,\n",
       "          [0.0718, 0.0815, 0.0912,  ..., 0.0386, 0.0313, 0.0239],\n",
       "          [0.0563, 0.0675, 0.0787,  ..., 0.0235, 0.0278, 0.0320],\n",
       "          [0.0409, 0.0535, 0.0662,  ..., 0.0083, 0.0242, 0.0402]],\n",
       "\n",
       "         [[0.0734, 0.0804, 0.0873,  ..., 0.1074, 0.0844, 0.0615],\n",
       "          [0.0518, 0.0430, 0.0341,  ..., 0.0391, 0.0494, 0.0597],\n",
       "          [0.0302, 0.0056, 0.0000,  ..., 0.0000, 0.0143, 0.0578],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0422],\n",
       "          [0.0334, 0.0248, 0.0162,  ..., 0.0000, 0.0069, 0.0486],\n",
       "          [0.0785, 0.0555, 0.0324,  ..., 0.0000, 0.0155, 0.0550]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0168, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],\n",
       "          [0.0060, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0032, 0.0196,  ..., 0.0110, 0.0176, 0.0242],\n",
       "          [0.0000, 0.0000, 0.0257,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0319,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0476,  ..., 0.0467, 0.0105, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0225,  ..., 0.0214, 0.0085, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0169]]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((1,1,patch_size,patch_size))\n",
    "\n",
    "v.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a763a865",
   "metadata": {
    "id": "a763a865",
    "outputId": "9168409b-099c-4e2a-83fc-06220919b439"
   },
   "outputs": [],
   "source": [
    "#model = depth_model(num_classes=1).to('cuda')\n",
    "#model = resunet(n_class=1).to('cuda')\n",
    "model = vanilla_unet(n_class=1).to('cuda')\n",
    "\n",
    "#model = effunet(n_class=1).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49976db4",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "77245589ab5e45a5adcf2e370a151190",
      "1d75becdfe22406e8ac99ee0229a7892"
     ]
    },
    "id": "49976db4",
    "outputId": "9af91597-7ddd-43b9-ea2b-b84880e9d000"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/7255972.1.csgpu/ipykernel_32372/2012793294.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm.tqdm_notebook(range(num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed96776fa92442cafdbe14374af4cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/7255972.1.csgpu/ipykernel_32372/2012793294.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for images, depths in tqdm.tqdm_notebook(train_dataloader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4892bc0eba5246adae31dc70ca63762b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inputs/834a953a2d7446a89ea3bde1c2084b81-1617744150700003958.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     27\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, depths \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm_notebook(train_dataloader):\n\u001b[1;32m     30\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m     depths \u001b[38;5;241m=\u001b[39m depths\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/projectnb/cs585bp/nkono/.conda/envs/depth/lib/python3.10/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs585bp/nkono/.conda/envs/depth/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mMonocularDepthDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m depth_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m##no rgb, takes grayscale\u001b[39;00m\n\u001b[1;32m     15\u001b[0m depth \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(depth_path)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# randomly crop image and depth\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/cs585bp/nkono/.conda/envs/depth/lib/python3.10/site-packages/PIL/Image.py:3131\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3128\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3131\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3132\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inputs/834a953a2d7446a89ea3bde1c2084b81-1617744150700003958.png'"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters, dataset paths, and other configurations\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((patch_size, patch_size)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "train_dataset = MonocularDepthDataset(df, transform = transform)\n",
    "#val_dataset = MonocularDepthDataset(val_image_paths, val_depth_paths, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "criterion = DepthEstimationLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in tqdm.tqdm_notebook(range(num_epochs)):\n",
    "    #train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, depths in tqdm.tqdm_notebook(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        depths = depths.to(device)\n",
    "        depths /= 10587\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs[-1].float(), depths.float())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    #val_loss = validate(model, val_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294280fd",
   "metadata": {
    "id": "294280fd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
