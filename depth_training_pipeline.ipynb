{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "38b88fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "from torchvision import models\n",
    "import tqdm\n",
    "\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize, Compose, ToPILImage, ToTensor\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from kornia.filters import SpatialGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d30c88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonocularDepthDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.image_paths = df.iloc[idx]['image']\n",
    "        self.depth_paths = df.iloc[idx]['depth']\n",
    "\n",
    "        image = Image.open(self.image_paths).convert('RGB')\n",
    "        depth = Image.open(self.depth_paths)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            depth = self.transform(depth)\n",
    "\n",
    "        return image, depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dd2d69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gradient_loss_fn(gen_frames, gt_frames, alpha=1):\n",
    "    def gradient(x):\n",
    "        # idea from tf.image.image_gradients(image)\n",
    "        # https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/image_ops_impl.py#L3441-L3512\n",
    "        # x: (b,c,h,w), float32 or float64\n",
    "        # dx, dy: (b,c,h,w)\n",
    "\n",
    "        h_x = x.size()[-2]\n",
    "        w_x = x.size()[-1]\n",
    "        # gradient step=1\n",
    "        left = x\n",
    "        right = F.pad(x, [0, 1, 0, 0])[:, :, :, 1:]\n",
    "        top = x\n",
    "        bottom = F.pad(x, [0, 0, 0, 1])[:, :, 1:, :]\n",
    "\n",
    "        # dx, dy = torch.abs(right - left), torch.abs(bottom - top)\n",
    "        dx, dy = right - left, bottom - top \n",
    "        # dx will always have zeros in the last column, right-left\n",
    "        # dy will always have zeros in the last row,    bottom-top\n",
    "        dx[:, :, :, -1] = 0\n",
    "        dy[:, :, -1, :] = 0\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "    # gradient\n",
    "    gen_dx, gen_dy = gradient(gen_frames)\n",
    "    gt_dx, gt_dy = gradient(gt_frames)\n",
    "    #\n",
    "    grad_diff_x = torch.abs(gt_dx - gen_dx)\n",
    "    grad_diff_y = torch.abs(gt_dy - gen_dy)\n",
    "\n",
    "    # condense into one tensor and avg\n",
    "    return torch.mean(grad_diff_x ** alpha + grad_diff_y ** alpha)\n",
    "\n",
    "class DepthEstimationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(DepthEstimationLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def forward(self, pred_depth, true_depth):\n",
    "        pred_depth = torch.clamp(pred_depth, min=1e-8)\n",
    "        true_depth = torch.clamp(true_depth, min=1e-8)\n",
    "\n",
    "        # Scale-invariant MSE loss\n",
    "        diff = torch.log(pred_depth) - torch.log(true_depth)\n",
    "        mse_loss = torch.mean(diff**2)\n",
    "        scale_invariant_mse_loss = mse_loss - (self.alpha * (torch.sum(diff)**2)) / (true_depth.numel()**2)\n",
    "\n",
    "    \n",
    "\n",
    "        gradient_loss = gradient_loss_fn(pred_depth,true_depth,alpha=self.alpha)\n",
    "\n",
    "        total_loss = (scale_invariant_mse_loss + gradient_loss)/2\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "47fa8a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class depth_model(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(depth_model, self).__init__()\n",
    "        self.base_model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Encoder layers (downsampling)\n",
    "        self.encoder1 = self.base_model._conv_stem\n",
    "        self.encoder2 = self.base_model._blocks[:2]\n",
    "        self.encoder3 = self.base_model._blocks[2:5]\n",
    "        self.encoder4 = self.base_model._blocks[5:12]\n",
    "        self.encoder5 = self.base_model._blocks[12:]\n",
    "        \n",
    "        # Decoder layers (upsampling)\n",
    "        self.decoder1 = self.decoder_block(320, 256)\n",
    "        self.decoder2 = self.decoder_block(256, 128)\n",
    "        self.decoder3 = self.decoder_block(128, 64)\n",
    "        self.decoder4 = self.decoder_block(64, 32)\n",
    "        \n",
    "        # Output layers for each decoder stage\n",
    "        self.output1 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        self.output2 = nn.Conv2d(128, num_classes, kernel_size=1)\n",
    "        self.output3 = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        self.output4 = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    def decoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self._encode_block(e1, self.encoder2)\n",
    "        e3 = self._encode_block(e2, self.encoder3)\n",
    "        e4 = self._encode_block(e3, self.encoder4)\n",
    "        e5 = self._encode_block(e4, self.encoder5)\n",
    "        \n",
    "        # Decoder\n",
    "        print(self.decoder1(e5).shape,e4.shape)\n",
    "        d1 = self.decoder1(e5)\n",
    "        d1 = self.resize_and_add(d1, e4)\n",
    "        d2 = self.decoder2(d1)\n",
    "        d2 = self.resize_and_add(d2, e3)\n",
    "        d3 = self.decoder3(d2)\n",
    "        d3 = self.resize_and_add(d3, e2)\n",
    "        d4 = self.decoder4(d3)\n",
    "        d4 = self.resize_and_add(d4, e1)\n",
    "        \n",
    "        # Output for each decoder stage\n",
    "        out1 = self.output1(d1)\n",
    "        out2 = self.output2(d2)\n",
    "        out3 = self.output3(d3)\n",
    "        out4 = self.output4(d4)\n",
    "\n",
    "        # Resize output to the same shape\n",
    "        H, W = x.size(2), x.size(3)\n",
    "        out1 = F.interpolate(out1, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        out2 = F.interpolate(out2, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        out3 = F.interpolate(out3, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        out4 = F.interpolate(out4, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Output: average of all the outputs\n",
    "        out_avg = (out1 + out2 + out3 + out4) / 4.0\n",
    "\n",
    "        return out1, out2, out3, out4, out_avg\n",
    "\n",
    "\n",
    "    def _encode_block(self, x, block):\n",
    "        for layer in block:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def resize_and_add(self, x1, x2):\n",
    "        x1_size = (x2.size(2), x2.size(3))\n",
    "        x1_resized = F.interpolate(x1, size=x1_size, mode='bilinear', align_corners=False)\n",
    "        return x1_resized + x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5adc8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu_block(in_channel,out_channel,kernel,padding):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_channel,out_channel, kernel_size = kernel, padding=padding),\n",
    "            nn.ReLU()) #nn.ReLU(inplace=True) #nn.Ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7a5c269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resmnet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        import torch.nn as nn\n",
    "\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "\n",
    "        self.input_1 = conv_relu_block(3, 64, 3, 1)\n",
    "        self.input_2 = conv_relu_block(64, 64, 3, 1)\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        \n",
    "        self.block0_1_conv = conv_relu_block(67,64,1,0)\n",
    "        self.block2_conv = conv_relu_block(131,128,1,0)\n",
    "\n",
    "        \n",
    "        self.l0 = nn.Sequential(*self.base_layers[:3])\n",
    "        self.U0_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up0 = conv_relu_block(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.l1 = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.U1_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up1 = conv_relu_block(64 + 256, 256, 3, 1)\n",
    "\n",
    "        self.l2 = self.base_layers[5]\n",
    "        self.U2_conv = conv_relu_block(128, 128, 1, 0)\n",
    "        self.conv_up2 = conv_relu_block(128 + 512, 256, 3, 1)\n",
    "\n",
    "        self.l3 = self.base_layers[6]\n",
    "        self.U3_conv = conv_relu_block(256, 256, 1, 0)\n",
    "        self.conv_up3 = conv_relu_block(256 + 512, 512, 3, 1)\n",
    "\n",
    "        self.l4 = self.base_layers[7]\n",
    "        self.U4_conv = conv_relu_block(512, 512, 1, 0)\n",
    "\n",
    "        self.conv_up4 = conv_relu_block(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.out1 = nn.Conv2d(512, n_class, 1)\n",
    "        self.out2 = nn.Conv2d(256, n_class, 1)\n",
    "        self.out3 = nn.Conv2d(256, n_class, 1)\n",
    "        self.out4 = nn.Conv2d(128, n_class, 1)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_one = self.input_1(x)\n",
    "        x_one = self.input_2(x_one)\n",
    "        \n",
    "        \n",
    "        scale_img_2 = self.pool(x)\n",
    "        scale_img_3 = self.pool(scale_img_2)\n",
    "        scale_img_4 = self.pool(scale_img_3)\n",
    "\n",
    "        block0 = self.l0(x)\n",
    "        block0 = torch.concatenate([block0,scale_img_2],axis=1)\n",
    "        block0 = self.block0_1_conv(block0)\n",
    "\n",
    "        block1 = self.l1(block0)\n",
    "        block1 = torch.concatenate([block1,scale_img_3],axis=1)\n",
    "        block1 = self.block0_1_conv(block1)\n",
    "\n",
    "        block2 = self.l2(block1)\n",
    "        block2 = torch.concatenate([block2,scale_img_4],axis=1)\n",
    "        block2 = self.block2_conv(block2)\n",
    "\n",
    "        print(block2.shape)\n",
    "\n",
    "        \n",
    "        block3 = self.l3(block2)\n",
    "        block4 = self.l4(block3)\n",
    "\n",
    "        block4 = self.U4_conv(block4)\n",
    "        x = self.upsample(block4)\n",
    "        block3 = self.U3_conv(block3)\n",
    "        x = torch.cat([x, block3], axis=1)\n",
    "        x = self.conv_up3(x)\n",
    "        out1 = self.out1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        block2 = self.U2_conv(block2)\n",
    "        x = torch.cat([x, block2], axis=1)\n",
    "        x = self.conv_up2(x)\n",
    "        out2 = self.out2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        block1 = self.U1_conv(block1)\n",
    "        x = torch.cat([x, block1], axis=1)\n",
    "        x = self.conv_up1(x)\n",
    "        out3 = self.out3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        block0 = self.U0_conv(block0)\n",
    "        x = torch.cat([x, block0], axis=1)\n",
    "        x = self.conv_up0(x)\n",
    "        out4 = self.out4(x)\n",
    "\n",
    "\n",
    "        out1_upsampled = F.interpolate(out1, scale_factor=16, mode='bilinear', align_corners=True)\n",
    "        out2_upsampled = F.interpolate(out2, scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        out3_upsampled = F.interpolate(out3, scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        out4_upsampled = F.interpolate(out4, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "\n",
    "\n",
    "        avg_out = (out1_upsampled + out2_upsampled + out3_upsampled + out4_upsampled) / 4\n",
    "\n",
    "        return out1, out2, out3, out4, avg_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9a171402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resunet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.input_1 = conv_relu_block(3, 64, 3, 1)\n",
    "        self.input_2 = conv_relu_block(64, 64, 3, 1)\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.l0 = nn.Sequential(*self.base_layers[:3])\n",
    "        self.U0_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up0 = conv_relu_block(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.l1 = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.U1_conv = conv_relu_block(64, 64, 1, 0)\n",
    "        self.conv_up1 = conv_relu_block(64 + 256, 256, 3, 1)\n",
    "\n",
    "        self.l2 = self.base_layers[5]\n",
    "        self.U2_conv = conv_relu_block(128, 128, 1, 0)\n",
    "        self.conv_up2 = conv_relu_block(128 + 512, 256, 3, 1)\n",
    "\n",
    "        self.l3 = self.base_layers[6]\n",
    "        self.U3_conv = conv_relu_block(256, 256, 1, 0)\n",
    "        self.conv_up3 = conv_relu_block(256 + 512, 512, 3, 1)\n",
    "\n",
    "        self.l4 = self.base_layers[7]\n",
    "        self.U4_conv = conv_relu_block(512, 512, 1, 0)\n",
    "\n",
    "        self.conv_up4 = conv_relu_block(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.out1 = nn.Conv2d(512, n_class, 1)\n",
    "        self.out2 = nn.Conv2d(256, n_class, 1)\n",
    "        self.out3 = nn.Conv2d(256, n_class, 1)\n",
    "        self.out4 = nn.Conv2d(128, n_class, 1)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_one = self.input_1(x)\n",
    "        x_one = self.input_2(x_one)\n",
    "\n",
    "        block0 = self.l0(x)\n",
    "        block1 = self.l1(block0)\n",
    "        block2 = self.l2(block1)\n",
    "        block3 = self.l3(block2)\n",
    "        block4 = self.l4(block3)\n",
    "\n",
    "        block4 = self.U4_conv(block4)\n",
    "        x = self.upsample(block4)\n",
    "        block3 = self.U3_conv(block3)\n",
    "        x = torch.cat([x, block3], axis=1)\n",
    "        x = self.conv_up3(x)\n",
    "        out1 = self.out1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        block2 = self.U2_conv(block2)\n",
    "        x = torch.cat([x, block2], axis=1)\n",
    "        x = self.conv_up2(x)\n",
    "        out2 = self.out2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        block1 = self.U1_conv(block1)\n",
    "        x = torch.cat([x, block1], axis=1)\n",
    "        x = self.conv_up1(x)\n",
    "        out3 = self.out3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        block0 = self.U0_conv(block0)\n",
    "        x = torch.cat([x, block0], axis=1)\n",
    "        x = self.conv_up0(x)\n",
    "        out4 = self.out4(x)\n",
    "\n",
    "\n",
    "        out1_upsampled = F.interpolate(out1, scale_factor=16, mode='bilinear', align_corners=True)\n",
    "        out2_upsampled = F.interpolate(out2, scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        out3_upsampled = F.interpolate(out3, scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        out4_upsampled = F.interpolate(out4, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "\n",
    "\n",
    "        avg_out = (out1_upsampled + out2_upsampled + out3_upsampled + out4_upsampled) / 4\n",
    "\n",
    "        return out1, out2, out3, out4, avg_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a763a865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\miniconda3\\envs\\carla\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\krish\\miniconda3\\envs\\carla\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#model = depth_model(num_classes=1).to('cuda')\n",
    "#model = resunet(n_class=1).to('cuda')\n",
    "model = resmnet(n_class=1).to('cuda')\n",
    "\n",
    "#model = effunet(n_class=1).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49976db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_8516\\1038386376.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm.tqdm_notebook(range(num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77245589ab5e45a5adcf2e370a151190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_8516\\1038386376.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for images, depths in tqdm.tqdm_notebook(train_dataloader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d75becdfe22406e8ac99ee0229a7892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 32, 32])\n",
      "torch.Size([8, 128, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters, dataset paths, and other configurations\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "train_dataset = MonocularDepthDataset(df, transform = transform)\n",
    "#val_dataset = MonocularDepthDataset(val_image_paths, val_depth_paths, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "criterion = DepthEstimationLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in tqdm.tqdm_notebook(range(num_epochs)):\n",
    "    #train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, depths in tqdm.tqdm_notebook(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        depths = depths.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs[-1].float(), depths.float())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(dataloader)\n",
    "    #val_loss = validate(model, val_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294280fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
